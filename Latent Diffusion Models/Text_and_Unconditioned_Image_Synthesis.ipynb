{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geetha495/Deep-Learning-Project/blob/main/Latent%20Diffusion%20Models/Text_and_Unconditioned_Image_Synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The notebook contains Text-to-Image Synthesis and Unconditional Image Synthesis."
      ],
      "metadata": {
        "id": "WXETqwYIiN28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Clone the repository"
      ],
      "metadata": {
        "id": "P_WBb8k5Bxwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VZD77bfadhp",
        "outputId": "9408cc11-8591-4a5c-a658-59d696e31c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stable-diffusion'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 42.65 MiB | 15.72 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CompVis/stable-diffusion.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Replace pytorch_lightning.utilities.distributed with pytorch_lightning.utilities.rank_zero"
      ],
      "metadata": {
        "id": "V0zagXvxCfRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Clone the taming transformers repository"
      ],
      "metadata": {
        "id": "x7VME8D7B-TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')"
      ],
      "metadata": {
        "id": "4zFw0AWUIQJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973265e1-afe4-4c27-89e1-537fde644727"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1342, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 1342 (delta 0), reused 1 (delta 0), pack-reused 1340\u001b[K\n",
            "Receiving objects: 100% (1342/1342), 409.77 MiB | 15.65 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n",
            "Obtaining file:///content/taming-transformers\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->taming-transformers==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->taming-transformers==0.0.1) (1.3.0)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Remove torch._six"
      ],
      "metadata": {
        "id": "Pc3AcsQSCGxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from taming.models import vqgan"
      ],
      "metadata": {
        "id": "MRd8dhys2B6u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stable-diffusion && pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EchOlifIXzQD",
        "outputId": "fbda3c7a-446d-4922-a093-2a656168cbe5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/stable-diffusion\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->latent-diffusion==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->latent-diffusion==0.0.1) (1.3.0)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the Model trained on LAION 400M"
      ],
      "metadata": {
        "id": "D2C8QhnOC6G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p stable-diffusion/models/ldm/text2img-large/\n",
        "!wget -O stable-diffusion/models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vm4kZxVaoFh",
        "outputId": "a5590312-cd45-4af9-e55e-dfcf6587f0e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 12:42:14--  https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n",
            "Resolving ommer-lab.com (ommer-lab.com)... 141.84.41.65\n",
            "Connecting to ommer-lab.com (ommer-lab.com)|141.84.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6152314307 (5.7G)\n",
            "Saving to: ‘stable-diffusion/models/ldm/text2img-large/model.ckpt’\n",
            "\n",
            "stable-diffusion/mo 100%[===================>]   5.73G  21.6MB/s    in 4m 47s  \n",
            "\n",
            "2023-11-24 12:47:01 (20.4 MB/s) - ‘stable-diffusion/models/ldm/text2img-large/model.ckpt’ saved [6152314307/6152314307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.19.2 scann kornia==0.6.4 torchmetrics==0.6.0\n",
        "!pip install git+https://github.com/arogozhnikov/einops.git"
      ],
      "metadata": {
        "id": "JKvjhatpdnt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5204672f-dba7-4780-b9aa-bfa83c4614c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.19.2\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scann\n",
            "  Downloading scann-1.2.10-cp310-cp310-manylinux_2_27_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia==0.6.4\n",
            "  Downloading kornia-0.6.4-py2.py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.4/493.4 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==0.6.0\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.2)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from kornia==0.6.4) (2.1.0+cu118)\n",
            "Collecting tensorflow~=2.13.0 (from scann)\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow~=2.13.0->scann)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow~=2.13.0->scann)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow~=2.13.0->scann)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow~=2.13.0->scann)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->scann) (0.34.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->kornia==0.6.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->kornia==0.6.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->kornia==0.6.4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->kornia==0.6.4) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (2023.7.22)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->scann) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->kornia==0.6.4) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->kornia==0.6.4) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->scann) (3.2.2)\n",
            "Installing collected packages: tokenizers, tensorflow-estimator, keras, gast, transformers, torchmetrics, kornia, tensorboard, tensorflow, scann\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 1.2.0\n",
            "    Uninstalling torchmetrics-1.2.0:\n",
            "      Successfully uninstalled torchmetrics-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytorch-lightning 2.1.2 requires torchmetrics>=0.7.0, but you have torchmetrics 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 keras-2.13.1 kornia-0.6.4 scann-1.2.10 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tokenizers-0.12.1 torchmetrics-0.6.0 transformers-4.19.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "torchmetrics",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/arogozhnikov/einops.git\n",
            "  Cloning https://github.com/arogozhnikov/einops.git to /tmp/pip-req-build-b_72pqzd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/arogozhnikov/einops.git /tmp/pip-req-build-b_72pqzd\n",
            "  Resolved https://github.com/arogozhnikov/einops.git to commit a6e93530ec2dce44f473e6065fad4d8236cda4f3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf pytorch-lightning"
      ],
      "metadata": {
        "id": "Z4QfyVM4OaHr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "02c04649-365f-4aa6-9809-2ff44fbeb5dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Using cached torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 0.6.0\n",
            "    Uninstalling torchmetrics-0.6.0:\n",
            "      Successfully uninstalled torchmetrics-0.6.0\n",
            "Successfully installed torchmetrics-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchmetrics"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip invisible-watermark diffusers transformers==4.19.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5P_Vsos2_8v",
        "outputId": "eaa48b87-bd0b-4078-a3a4-f0aed5c55e69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clip\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting invisible-watermark\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.23.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers==4.19.2 in /usr/local/lib/python3.10/dist-packages (4.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (4.66.1)\n",
            "Requirement already satisfied: Pillow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (9.4.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (1.4.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (2.1.0+cu118)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.8.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->invisible-watermark) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible-watermark) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6990 sha256=3b5c820c510da345c9f7aa0b433b3316103c79d6f892590691778c351046a5d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/5c/e6/2c0fdb453a3569188864b17e9676bea8b3b7e160c037117869\n",
            "Successfully built clip\n",
            "Installing collected packages: clip, invisible-watermark, diffusers\n",
            "Successfully installed clip-0.2.0 diffusers-0.23.1 invisible-watermark-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "72tUHwwlDIJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd stable-diffusion && python scripts/txt2img.py --prompt \"a sunset behind a mountain range, vector image\" --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0"
      ],
      "metadata": {
        "id": "k5jLGO4L0jS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f62175f-29b1-4b5a-83bf-17b30c9cf1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from models/ldm/text2img-large/model.ckpt\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 872.30 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Sampling:   0% 0/1 [00:00<?, ?it/s]Data shape for DDIM sampling is (1, 4, 48, 128), eta 1.0\n",
            "Running DDIM Sampling with 200 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   0% 1/200 [00:02<06:57,  2.10s/it]\u001b[A\n",
            "DDIM Sampler:   1% 2/200 [00:03<04:41,  1.42s/it]\u001b[A\n",
            "DDIM Sampler:   2% 3/200 [00:04<03:59,  1.21s/it]\u001b[A\n",
            "DDIM Sampler:   2% 4/200 [00:04<03:38,  1.11s/it]\u001b[A\n",
            "DDIM Sampler:   2% 5/200 [00:05<03:26,  1.06s/it]\u001b[A\n",
            "DDIM Sampler:   3% 6/200 [00:06<03:19,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:   4% 7/200 [00:07<03:14,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:   4% 8/200 [00:08<03:10,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:   4% 9/200 [00:09<03:08,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:   5% 10/200 [00:10<03:06,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:   6% 11/200 [00:11<03:04,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:   6% 12/200 [00:12<03:03,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   6% 13/200 [00:13<03:01,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   7% 14/200 [00:14<03:00,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   8% 15/200 [00:15<02:59,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   8% 16/200 [00:16<02:58,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   8% 17/200 [00:17<02:57,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:   9% 18/200 [00:18<02:56,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  10% 19/200 [00:19<02:55,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  10% 20/200 [00:20<02:54,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  10% 21/200 [00:21<02:53,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  11% 22/200 [00:22<02:53,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  12% 23/200 [00:23<02:52,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  12% 24/200 [00:24<02:51,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  12% 25/200 [00:25<02:50,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  13% 26/200 [00:26<02:49,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  14% 27/200 [00:27<02:48,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  14% 28/200 [00:28<02:47,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  14% 29/200 [00:29<02:46,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  15% 30/200 [00:30<02:45,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  16% 31/200 [00:31<02:44,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  16% 32/200 [00:32<02:43,  1.03it/s]\u001b[A\n",
            "DDIM Sampler:  16% 33/200 [00:33<02:43,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  17% 34/200 [00:34<02:42,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  18% 35/200 [00:35<02:41,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  18% 36/200 [00:36<02:40,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  18% 37/200 [00:37<02:39,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  19% 38/200 [00:38<02:38,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  20% 39/200 [00:39<02:37,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  20% 40/200 [00:40<02:37,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  20% 41/200 [00:40<02:36,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  21% 42/200 [00:41<02:35,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  22% 43/200 [00:42<02:34,  1.02it/s]\u001b[A\n",
            "DDIM Sampler:  22% 44/200 [00:43<02:33,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  22% 45/200 [00:44<02:32,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  23% 46/200 [00:45<02:31,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  24% 47/200 [00:46<02:31,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  24% 48/200 [00:47<02:30,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  24% 49/200 [00:48<02:29,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  25% 50/200 [00:49<02:28,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  26% 51/200 [00:50<02:27,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  26% 52/200 [00:51<02:27,  1.01it/s]\u001b[A\n",
            "DDIM Sampler:  26% 53/200 [00:52<02:26,  1.00it/s]\u001b[A\n",
            "DDIM Sampler:  27% 54/200 [00:53<02:25,  1.00it/s]\u001b[A\n",
            "DDIM Sampler:  28% 55/200 [00:54<02:24,  1.00it/s]\u001b[A\n",
            "DDIM Sampler:  28% 56/200 [00:55<02:23,  1.00it/s]\u001b[A\n",
            "DDIM Sampler:  28% 57/200 [00:56<02:22,  1.00it/s]\u001b[A\n",
            "DDIM Sampler:  29% 58/200 [00:57<02:22,  1.00s/it]\u001b[A\n",
            "DDIM Sampler:  30% 59/200 [00:58<02:21,  1.00s/it]\u001b[A\n",
            "DDIM Sampler:  30% 60/200 [00:59<02:20,  1.00s/it]\u001b[A\n",
            "DDIM Sampler:  30% 61/200 [01:00<02:19,  1.00s/it]\u001b[A\n",
            "DDIM Sampler:  31% 62/200 [01:01<02:18,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  32% 63/200 [01:02<02:18,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  32% 64/200 [01:03<02:17,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  32% 65/200 [01:04<02:16,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  33% 66/200 [01:05<02:15,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  34% 67/200 [01:06<02:14,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  34% 68/200 [01:08<02:13,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  34% 69/200 [01:09<02:13,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  35% 70/200 [01:10<02:12,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  36% 71/200 [01:11<02:11,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  36% 72/200 [01:12<02:10,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  36% 73/200 [01:13<02:10,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  37% 74/200 [01:14<02:09,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  38% 75/200 [01:15<02:08,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  38% 76/200 [01:16<02:07,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  38% 77/200 [01:17<02:06,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  39% 78/200 [01:18<02:05,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  40% 79/200 [01:19<02:05,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  40% 80/200 [01:20<02:04,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  40% 81/200 [01:21<02:03,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  41% 82/200 [01:22<02:02,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  42% 83/200 [01:23<02:01,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  42% 84/200 [01:24<02:00,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  42% 85/200 [01:25<01:59,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  43% 86/200 [01:26<01:58,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  44% 87/200 [01:27<01:57,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  44% 88/200 [01:28<01:56,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  44% 89/200 [01:29<01:55,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  45% 90/200 [01:30<01:54,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  46% 91/200 [01:31<01:53,  1.04s/it]\u001b[A\n",
            "DDIM Sampler:  46% 92/200 [01:32<01:51,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  46% 93/200 [01:33<01:50,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  47% 94/200 [01:34<01:49,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  48% 95/200 [01:35<01:48,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  48% 96/200 [01:36<01:46,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  48% 97/200 [01:37<01:45,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  49% 98/200 [01:39<01:44,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  50% 99/200 [01:40<01:43,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  50% 100/200 [01:41<01:42,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  50% 101/200 [01:42<01:41,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  51% 102/200 [01:43<01:40,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  52% 103/200 [01:44<01:39,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  52% 104/200 [01:45<01:37,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  52% 105/200 [01:46<01:36,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  53% 106/200 [01:47<01:35,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  54% 107/200 [01:48<01:34,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  54% 108/200 [01:49<01:33,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  55% 109/200 [01:50<01:32,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  55% 110/200 [01:51<01:31,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  56% 111/200 [01:52<01:30,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  56% 112/200 [01:53<01:29,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  56% 113/200 [01:54<01:28,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  57% 114/200 [01:55<01:27,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  57% 115/200 [01:56<01:26,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  58% 116/200 [01:57<01:25,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  58% 117/200 [01:58<01:24,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  59% 118/200 [01:59<01:23,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  60% 119/200 [02:00<01:22,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  60% 120/200 [02:01<01:20,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  60% 121/200 [02:02<01:19,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  61% 122/200 [02:03<01:18,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  62% 123/200 [02:04<01:17,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  62% 124/200 [02:05<01:16,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  62% 125/200 [02:06<01:15,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  63% 126/200 [02:07<01:14,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  64% 127/200 [02:08<01:13,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  64% 128/200 [02:09<01:12,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  64% 129/200 [02:10<01:11,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  65% 130/200 [02:11<01:10,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  66% 131/200 [02:12<01:09,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  66% 132/200 [02:13<01:08,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  66% 133/200 [02:14<01:07,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  67% 134/200 [02:15<01:06,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  68% 135/200 [02:16<01:05,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  68% 136/200 [02:17<01:04,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  68% 137/200 [02:18<01:03,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  69% 138/200 [02:19<01:02,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  70% 139/200 [02:20<01:01,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  70% 140/200 [02:21<01:00,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  70% 141/200 [02:22<00:59,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  71% 142/200 [02:23<00:58,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  72% 143/200 [02:24<00:57,  1.01s/it]\u001b[A\n",
            "DDIM Sampler:  72% 144/200 [02:25<00:56,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  72% 145/200 [02:26<00:55,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  73% 146/200 [02:27<00:54,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  74% 147/200 [02:28<00:53,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  74% 148/200 [02:29<00:52,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  74% 149/200 [02:30<00:51,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  75% 150/200 [02:31<00:50,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  76% 151/200 [02:32<00:49,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  76% 152/200 [02:33<00:48,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  76% 153/200 [02:34<00:48,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  77% 154/200 [02:35<00:47,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  78% 155/200 [02:36<00:46,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  78% 156/200 [02:37<00:44,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  78% 157/200 [02:38<00:44,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  79% 158/200 [02:39<00:43,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  80% 159/200 [02:40<00:42,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  80% 160/200 [02:42<00:40,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  80% 161/200 [02:43<00:39,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  81% 162/200 [02:44<00:38,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  82% 163/200 [02:45<00:37,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  82% 164/200 [02:46<00:36,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  82% 165/200 [02:47<00:35,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  83% 166/200 [02:48<00:34,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  84% 167/200 [02:49<00:33,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  84% 168/200 [02:50<00:32,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  84% 169/200 [02:51<00:31,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  85% 170/200 [02:52<00:30,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  86% 171/200 [02:53<00:29,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  86% 172/200 [02:54<00:28,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  86% 173/200 [02:55<00:27,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  87% 174/200 [02:56<00:26,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  88% 175/200 [02:57<00:25,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  88% 176/200 [02:58<00:24,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  88% 177/200 [02:59<00:23,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  89% 178/200 [03:00<00:22,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  90% 179/200 [03:01<00:21,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  90% 180/200 [03:02<00:20,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  90% 181/200 [03:03<00:19,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  91% 182/200 [03:04<00:18,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  92% 183/200 [03:05<00:17,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  92% 184/200 [03:06<00:16,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  92% 185/200 [03:07<00:15,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  93% 186/200 [03:08<00:14,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  94% 187/200 [03:09<00:13,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  94% 188/200 [03:10<00:12,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  94% 189/200 [03:11<00:11,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  95% 190/200 [03:12<00:10,  1.03s/it]\u001b[A\n",
            "DDIM Sampler:  96% 191/200 [03:13<00:09,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  96% 192/200 [03:14<00:08,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  96% 193/200 [03:15<00:07,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  97% 194/200 [03:16<00:06,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  98% 195/200 [03:17<00:05,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  98% 196/200 [03:18<00:04,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  98% 197/200 [03:19<00:03,  1.02s/it]\u001b[A\n",
            "DDIM Sampler:  99% 198/200 [03:20<00:02,  1.02s/it]\u001b[A\n",
            "DDIM Sampler: 100% 199/200 [03:21<00:01,  1.02s/it]\u001b[A\n",
            "DDIM Sampler: 100% 200/200 [03:23<00:00,  1.02s/it]\n",
            "Sampling: 100% 1/1 [03:24<00:00, 204.20s/it]\n",
            "Your samples are ready and waiting four you here: \n",
            "outputs/txt2img-samples \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading model trained on LAION 5B"
      ],
      "metadata": {
        "id": "7q2JTtQkDPSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p stable-diffusion/models/ldm/stable-diffusion-v4-ema/\n",
        "!wget -O stable-diffusion/models/ldm/stable-diffusion-v4-ema/model.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R3KhbvpGj8M",
        "outputId": "8e02495d-b780-4986-dd47-e35be586cc1d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 13:05:08--  https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.33.110, 13.33.33.55, 13.33.33.20, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.33.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/4c/37/4c372b4ebb57bbd02e68413d4951aa326d4b3cfb6e62db989e529c6d4b26fb21/14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sd-v1-4-full-ema.ckpt%3B+filename%3D%22sd-v1-4-full-ema.ckpt%22%3B&Expires=1701090308&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTA5MDMwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Yy8zNy80YzM3MmI0ZWJiNTdiYmQwMmU2ODQxM2Q0OTUxYWEzMjZkNGIzY2ZiNmU2MmRiOTg5ZTUyOWM2ZDRiMjZmYjIxLzE0NzQ5ZWZjMGFlOGVmMDMyOTM5MWFkNDQzNmZlYjc4MWI0MDJmNGZlY2U0ODgzYzdhZDhkMTA1NTZkOGEzNmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RzZCT8o73BBO9UswzYUC645A7wIX0Y%7E92rb7C61i9isWJEjqkNK4gOrR1mE6Be8qHiVUMKNlk00IteKWX1JnV4rgebn2OsA59SHeURwJxTv86Rzly2y0J91IARowy3OItrlGhZ4-c0PAOCg3Wv2hLwj69OTJj72YZTYqbpaaH46%7EAsaMdPmAu0oAfpPXIAlxKBUE6fFTPXVReK3R8xdHgu4s4ZbTuI-dL6oLa-2cRY%7EXhkDuZVPp-JVC9nhYfBjPtATU6NkIc4sx526%7EgdTb1ORUwVTVvdi9CQf6E3y7NtfzLX7uQ7k49bMoblrANpcgLyL3afe-yf34UFlJE7aVcA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-24 13:05:08--  https://cdn-lfs.huggingface.co/repos/4c/37/4c372b4ebb57bbd02e68413d4951aa326d4b3cfb6e62db989e529c6d4b26fb21/14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sd-v1-4-full-ema.ckpt%3B+filename%3D%22sd-v1-4-full-ema.ckpt%22%3B&Expires=1701090308&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTA5MDMwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Yy8zNy80YzM3MmI0ZWJiNTdiYmQwMmU2ODQxM2Q0OTUxYWEzMjZkNGIzY2ZiNmU2MmRiOTg5ZTUyOWM2ZDRiMjZmYjIxLzE0NzQ5ZWZjMGFlOGVmMDMyOTM5MWFkNDQzNmZlYjc4MWI0MDJmNGZlY2U0ODgzYzdhZDhkMTA1NTZkOGEzNmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RzZCT8o73BBO9UswzYUC645A7wIX0Y%7E92rb7C61i9isWJEjqkNK4gOrR1mE6Be8qHiVUMKNlk00IteKWX1JnV4rgebn2OsA59SHeURwJxTv86Rzly2y0J91IARowy3OItrlGhZ4-c0PAOCg3Wv2hLwj69OTJj72YZTYqbpaaH46%7EAsaMdPmAu0oAfpPXIAlxKBUE6fFTPXVReK3R8xdHgu4s4ZbTuI-dL6oLa-2cRY%7EXhkDuZVPp-JVC9nhYfBjPtATU6NkIc4sx526%7EgdTb1ORUwVTVvdi9CQf6E3y7NtfzLX7uQ7k49bMoblrANpcgLyL3afe-yf34UFlJE7aVcA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.33.33.69, 13.33.33.93, 13.33.33.45, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.33.33.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7703807346 (7.2G) [binary/octet-stream]\n",
            "Saving to: ‘stable-diffusion/models/ldm/stable-diffusion-v4-ema/model.ckpt’\n",
            "\n",
            "stable-diffusion/mo 100%[===================>]   7.17G   121MB/s    in 86s     \n",
            "\n",
            "2023-11-24 13:06:34 (85.9 MB/s) - ‘stable-diffusion/models/ldm/stable-diffusion-v4-ema/model.ckpt’ saved [7703807346/7703807346]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "--1YWLhFDrbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stable-diffusion && python scripts/txt2img.py --prompt \"a happy beautiful white-haired anime wolf girl wearing purple kimono, wavy hair, anime, beautiful face, portrait, soft lighting, blur, flare, detailed accessories, graceful pose, detailed face, detailed kimono, detailed Japanese building background, hyperdetailed, masterpiece, 4k, 8k, trending on pixiv,\" --plms --n_iter=1 --n_samples=1 --skip_grid --ckpt=models/ldm/stable-diffusion-v4-ema/model.ckpt --config=configs/stable-diffusion/v1-inference.yaml --seed=18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQwbKLrgLS1Q",
        "outputId": "b43eacd0-e84d-4d64-8eee-3174978adcc7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 18\n",
            "Loading model from models/ldm/stable-diffusion-v4-ema/model.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'logit_scale', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.4.layer_norm2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/1 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:01<01:34,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:02<00:45,  1.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:02<00:29,  1.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:02<00:22,  2.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:03<00:18,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:03<00:15,  2.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:03<00:14,  3.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:03<00:13,  3.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:04<00:12,  3.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:04<00:11,  3.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:04<00:10,  3.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:04<00:10,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:05<00:10,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:05<00:09,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:05<00:09,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:05<00:09,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:06<00:08,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:06<00:08,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:06<00:08,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:06<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:07<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:07<00:07,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:07<00:07,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:08<00:06,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:08<00:06,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:08<00:06,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:08<00:06,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:09<00:05,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:09<00:05,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:09<00:05,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:09<00:05,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:10<00:04,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:10<00:04,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:10<00:04,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:10<00:04,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:11<00:03,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:11<00:03,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:11<00:03,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:12<00:02,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:12<00:02,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:12<00:02,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:12<00:02,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:13<00:01,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:13<00:01,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:13<00:01,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:13<00:01,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:14<00:00,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:14<00:00,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:14<00:00,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:14<00:00,  3.33it/s]\n",
            "\n",
            "data: 100% 1/1 [00:19<00:00, 19.45s/it]\n",
            "Sampling: 100% 1/1 [00:19<00:00, 19.45s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stable-diffusion && python scripts/txt2img.py --prompt \"a hyper realistic rilakkuma enjoying an icecream in the snow\" --plms --n_iter=1 --n_samples=1 --skip_grid --ckpt=models/ldm/stable-diffusion-v4-ema/model.ckpt --config=configs/stable-diffusion/v1-inference.yaml --seed=18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92YF5Ql5NCGg",
        "outputId": "c935a0b5-a3d7-43d1-d7ba-74edbc675244"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 18\n",
            "Loading model from models/ldm/stable-diffusion-v4-ema/model.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/1 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:01<01:26,  1.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:02<00:42,  1.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:02<00:28,  1.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:02<00:21,  2.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:02<00:17,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:03<00:15,  2.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:03<00:13,  3.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:03<00:12,  3.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:03<00:11,  3.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:04<00:11,  3.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:04<00:10,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:04<00:10,  3.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:04<00:09,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:05<00:09,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:05<00:09,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:05<00:09,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:05<00:08,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:06<00:08,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:06<00:08,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:06<00:07,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:07<00:07,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:07<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:07<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:07<00:06,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:08<00:06,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:08<00:06,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:08<00:06,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:08<00:05,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:09<00:05,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:09<00:05,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:09<00:04,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:09<00:04,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:10<00:04,  3.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:10<00:04,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:10<00:03,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:10<00:03,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:11<00:03,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:11<00:03,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:11<00:02,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:12<00:02,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:12<00:02,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:12<00:02,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:12<00:01,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:13<00:01,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:13<00:01,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:13<00:01,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:13<00:00,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:14<00:00,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:14<00:00,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:14<00:00,  3.40it/s]\n",
            "\n",
            "data: 100% 1/1 [00:18<00:00, 18.84s/it]\n",
            "Sampling: 100% 1/1 [00:18<00:00, 18.84s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf samples.tar.gz ./stable-diffusion/outputs/txt2img-samples/samples"
      ],
      "metadata": {
        "id": "HkdItiLATSoj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stable-diffusion && python scripts/txt2img.py --prompt \"A close up portrait of a beautiful woman picking flowers in a meadow by Ken Sugimori, summer, dawn, anime\"  --plms --n_iter=1 --n_samples=1 --skip_grid --ckpt=models/ldm/stable-diffusion-v4-ema/model.ckpt --config=configs/stable-diffusion/v1-inference.yaml --seed=18\n",
        "\n",
        "# !cd stable-diffusion && python scripts/txt2img.py --prompt \"creamcake with icecream, Watercolor, trending on artstation, sharp focus, studio photo, intricate details, highly detailed\"  --plms --n_iter=1 --n_samples=1 --skip_grid --ckpt=models/ldm/stable-diffusion-v4-ema/model.ckpt --config=configs/stable-diffusion/v1-inference.yaml --seed=18\n"
      ],
      "metadata": {
        "id": "Os7XPPYVVVdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde17a12-ebed-4edd-9ca1-c3539411cee0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 18\n",
            "Loading model from models/ldm/stable-diffusion-v4-ema/model.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'text_projection.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/1 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:01<01:29,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:02<00:43,  1.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:02<00:28,  1.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:02<00:21,  2.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:02<00:17,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:03<00:15,  2.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:03<00:13,  3.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:03<00:12,  3.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:03<00:11,  3.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:04<00:11,  3.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:04<00:10,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:04<00:10,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:04<00:09,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:05<00:09,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:05<00:09,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:05<00:08,  3.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:05<00:08,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:06<00:08,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:06<00:08,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:06<00:07,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:07<00:07,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:07<00:07,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:07<00:07,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:07<00:06,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:08<00:06,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:08<00:06,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:08<00:05,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:08<00:05,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:09<00:05,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:09<00:05,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:09<00:04,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:09<00:04,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:10<00:04,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:10<00:04,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:10<00:03,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:10<00:03,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:11<00:03,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:11<00:03,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:11<00:02,  3.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:11<00:02,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:12<00:02,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:12<00:02,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:12<00:01,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:13<00:01,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:13<00:01,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:13<00:01,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:13<00:00,  3.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:14<00:00,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:14<00:00,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:14<00:00,  3.43it/s]\n",
            "\n",
            "data: 100% 1/1 [00:17<00:00, 17.91s/it]\n",
            "Sampling: 100% 1/1 [00:17<00:00, 17.91s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional Image Synthesis"
      ],
      "metadata": {
        "id": "FaYRFghGBtPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItlFcW1wFQTs",
        "outputId": "219b5598-46e2-4e38-9778-834c98f02e3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 16:06:54--  https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip\n",
            "Resolving ommer-lab.com (ommer-lab.com)... 141.84.41.65\n",
            "Connecting to ommer-lab.com (ommer-lab.com)|141.84.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2246975121 (2.1G) [application/zip]\n",
            "Saving to: ‘lsun_bedrooms.zip’\n",
            "\n",
            "lsun_bedrooms.zip   100%[===================>]   2.09G  13.3MB/s    in 2m 43s  \n",
            "\n",
            "2023-11-24 16:09:38 (13.2 MB/s) - ‘lsun_bedrooms.zip’ saved [2246975121/2246975121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vLcOvxL3FPbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p stable-diffusion/models/models/ldm/lsun-beds256\n",
        "!unzip lsun_bedrooms.zip -d stable-diffusion/models/ldm/lsun_beds256/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ULdvgVKFi2D",
        "outputId": "8eb02e7f-5002-4de5-f976-ffffd26c0efa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lsun_bedrooms.zip\n",
            "  inflating: stable-diffusion/models/ldm/lsun-bedroom/model.ckpt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stable-diffusion && python scripts/sample_diffusion.py -r models/ldm/lsun_beds256/model.ckpt -n=10 --batch_size 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz6gxpkylqA-",
        "outputId": "922a3fb9-baac-47cb-df28-5a16a081002b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logdir is models/ldm/lsun_beds256\n",
            "{'model': {'base_learning_rate': 2e-06, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.0015, 'linear_end': 0.0195, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'class_label', 'image_size': 64, 'channels': 3, 'cond_stage_trainable': False, 'concat_mode': False, 'monitor': 'val/loss', 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 64, 'in_channels': 3, 'out_channels': 3, 'model_channels': 224, 'attention_resolutions': [8, 4, 2], 'num_res_blocks': 2, 'channel_mult': [1, 2, 3, 4], 'num_head_channels': 32}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.VQModelInterface', 'params': {'embed_dim': 3, 'n_embed': 8192, 'ddconfig': {'double_z': False, 'z_channels': 3, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': '__is_unconditional__'}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 48, 'num_workers': 5, 'wrap': False, 'train': {'target': 'ldm.data.lsun.LSUNBedroomsTrain', 'params': {'size': 256}}, 'validation': {'target': 'ldm.data.lsun.LSUNBedroomsValidation', 'params': {'size': 256}}}}}\n",
            "Loading model from models/ldm/lsun_beds256/model.ckpt\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 274.06 M params.\n",
            "Keeping EMAs of 370.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Training LatentDiffusion as an unconditional model.\n",
            "global step: 1900000\n",
            "===========================================================================\n",
            "logging to:\n",
            "models/ldm/lsun_beds256/samples/01900000/2023-11-24-16-34-50\n",
            "===========================================================================\n",
            "{'resume': 'models/ldm/lsun_beds256/model.ckpt', 'n_samples': 10, 'eta': 1.0, 'vanilla_sample': False, 'logdir': 'none', 'custom_steps': 50, 'batch_size': 10, 'base': ['models/ldm/lsun_beds256/config.yaml']}\n",
            "Using DDIM sampling with 50 sampling steps and eta=1.0\n",
            "Running unconditional sampling for 10 samples\n",
            "Sampling Batches (unconditional):   0% 0/1 [00:00<?, ?it/s]Plotting: Switched to EMA weights\n",
            "Data shape for DDIM sampling is (10, 3, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:27,  1.78it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:22,  2.17it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:01<00:20,  2.26it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:19,  2.35it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:02<00:18,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:02<00:18,  2.41it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:17,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:03<00:17,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:03<00:16,  2.44it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:04<00:16,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:04<00:16,  2.44it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:05<00:15,  2.44it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:05<00:15,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:05<00:14,  2.44it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:06<00:14,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:06<00:14,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:07<00:13,  2.43it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:07<00:13,  2.42it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:07<00:12,  2.42it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:08<00:12,  2.41it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:08<00:11,  2.42it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:09<00:11,  2.41it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:09<00:11,  2.41it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:10<00:10,  2.41it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:10<00:10,  2.40it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:10<00:10,  2.40it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:11<00:09,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:11<00:09,  2.40it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:12<00:08,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:12<00:08,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:13<00:07,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:13<00:07,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:14<00:06,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:14<00:06,  2.37it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:15<00:05,  2.37it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:15<00:05,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:15<00:05,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:16<00:04,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:16<00:04,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:17<00:03,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:17<00:03,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:17<00:02,  2.39it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:18<00:02,  2.38it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:18<00:02,  2.37it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:19<00:01,  2.36it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:19<00:01,  2.37it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:20<00:00,  2.37it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:20<00:00,  2.37it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:20<00:00,  2.39it/s]\n",
            "Plotting: Restored training weights\n",
            "Throughput for this batch: 0.4769930698521641\n",
            "Sampling Batches (unconditional): 100% 1/1 [00:22<00:00, 22.84s/it]\n",
            "sampling of 9 images finished in 0.38 minutes.\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "C6BN_FYIGebS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def concatenate_images(directory, output_path, rows, cols):\n",
        "    images = []\n",
        "\n",
        "    for filename in (os.listdir(directory)):\n",
        "          img_path = os.path.join(directory, filename)\n",
        "          img = Image.open(img_path)\n",
        "          images.append(img)\n",
        "\n",
        "    rows_images = [Image.new('RGB', (img.width * cols, img.height)) for img in images]\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            rows_images[i].paste(images[i * cols + j], (j * images[i].width, 0))\n",
        "\n",
        "    result = Image.new('RGB', (rows_images[0].width, rows_images[0].height * rows))\n",
        "\n",
        "    for i in range(rows):\n",
        "        result.paste(rows_images[i], (0, i * rows_images[i].height))\n",
        "\n",
        "    result.save(output_path)\n",
        "\n",
        "input_directory = \"/content/stable-diffusion/models/ldm/lsun_beds256/samples/01900000/2023-11-24-16-34-50/img\"\n",
        "output_image_path = \"concat.png\"\n",
        "rows_count = 2\n",
        "cols_count = 4\n",
        "\n",
        "concatenate_images(input_directory, output_image_path, rows_count, cols_count)\n"
      ],
      "metadata": {
        "id": "huIPj_i9ME4t"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRzbDrReMvvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}